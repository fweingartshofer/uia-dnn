{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "In this task you are suppose to implement a convolutional neural network using a high level library, e.g. PyTorch.\n",
    "The classification should be about food.\n",
    "* Download the food 11 dataset https://mmspg.epfl.ch/food-image-datasets or https://www.kaggle.com/vermaavi/food11/data\n",
    "* Predict the 11 classes: Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit\n",
    "* Try some standard networks convolutional networks before more complex ones.\n",
    "* Hint: Start with a subset of the dataset\n",
    "* Choose the network architecture with care.\n",
    "* Train and validate all algorithms.\n",
    "* Make the necessary assumptions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c642e20328c329a"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Load and preprocess the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "   def __init__(self, root_dir, transform=None):\n",
    "       self.root_dir = root_dir\n",
    "       self.transform = transform\n",
    "       self.file_list = os.listdir(root_dir)\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.file_list)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       img_name = os.path.join(self.root_dir, self.file_list[idx])\n",
    "       image = Image.open(img_name)\n",
    "       if self.transform:\n",
    "           image = self.transform(image)\n",
    "       label = int(self.file_list[idx].split(\"_\")[0])\n",
    "       return image, label\n",
    "\n",
    "\n",
    "train_dir = 'food-11/training'\n",
    "val_dir = 'food-11/validation'\n",
    "\n",
    "train_dataset = FoodDataset(train_dir, transform=transform)\n",
    "val_dataset = FoodDataset(val_dir, transform=transform)\n",
    "\n",
    "# Calculate the number of samples to use for training (10%)\n",
    "n_train = len(train_dataset)\n",
    "indices = list(range(n_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.1 * n_train))\n",
    "\n",
    "# Get the random indices for the subset\n",
    "train_idx = indices[:split]\n",
    "\n",
    "# Create a sampler\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:29:07.684224457Z",
     "start_time": "2023-09-13T09:29:07.643004081Z"
    }
   },
   "id": "99ad7db13e90cbc9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 2.096089 \tTraining Accuracy: 0.258565 \tTest Loss: 0.059631\n",
      "Epoch: 2 \tTraining Loss: 1.760275 \tTraining Accuracy: 0.378877 \tTest Loss: 0.054637\n",
      "Epoch: 3 \tTraining Loss: 1.516161 \tTraining Accuracy: 0.469795 \tTest Loss: 0.052070\n",
      "Epoch: 4 \tTraining Loss: 1.246069 \tTraining Accuracy: 0.567200 \tTest Loss: 0.053496\n",
      "Epoch: 5 \tTraining Loss: 0.908189 \tTraining Accuracy: 0.693594 \tTest Loss: 0.059498\n",
      "Epoch: 6 \tTraining Loss: 0.554893 \tTraining Accuracy: 0.820495 \tTest Loss: 0.075193\n",
      "Epoch: 7 \tTraining Loss: 0.276336 \tTraining Accuracy: 0.916785 \tTest Loss: 0.090224\n",
      "Epoch: 8 \tTraining Loss: 0.134404 \tTraining Accuracy: 0.962599 \tTest Loss: 0.110442\n",
      "Epoch: 9 \tTraining Loss: 0.071679 \tTraining Accuracy: 0.981350 \tTest Loss: 0.123245\n",
      "Epoch: 10 \tTraining Loss: 0.034728 \tTraining Accuracy: 0.992398 \tTest Loss: 0.142721\n",
      "Epoch: 11 \tTraining Loss: 0.067772 \tTraining Accuracy: 0.980032 \tTest Loss: 0.149828\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 46\u001B[0m\n\u001B[1;32m     43\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     45\u001B[0m     _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 46\u001B[0m     train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     47\u001B[0m     train_correct \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (predicted \u001B[38;5;241m==\u001B[39m labels)\u001B[38;5;241m.\u001B[39msum()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     49\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m train_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataset)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2: Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(64 * 30 * 30, 128)  # instead of nn.Linear(64 * 15 * 15, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Step 3: Train the CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_loss += loss\n",
    "            \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tTest Loss: {:.6f}'.format(\n",
    "        epoch + 1, train_loss, train_accuracy, test_loss / len(val_dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:40:27.177218334Z",
     "start_time": "2023-09-13T09:33:13.822217703Z"
    }
   },
   "id": "b6f8827c198ce458"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Bread       0.33      0.32      0.32       368\n",
      "  Dairy product       0.32      0.21      0.25       148\n",
      "        Dessert       0.35      0.38      0.36       500\n",
      "            Egg       0.34      0.27      0.30       335\n",
      "     Fried food       0.42      0.41      0.41       287\n",
      "           Meat       0.48      0.53      0.50       432\n",
      "  Noodles/Pasta       0.51      0.33      0.40       147\n",
      "           Rice       0.41      0.23      0.29        96\n",
      "        Seafood       0.40      0.50      0.44       303\n",
      "           Soup       0.59      0.70      0.64       500\n",
      "Vegetable/Fruit       0.76      0.62      0.68       231\n",
      "\n",
      "       accuracy                           0.45      3347\n",
      "      macro avg       0.44      0.41      0.42      3347\n",
      "   weighted avg       0.44      0.45      0.44      3347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "eval_dir = 'food-11/evaluation'\n",
    "eval_dataset = FoodDataset(eval_dir, transform=transform)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=32)\n",
    "\n",
    "# Step 4: Evaluate the CNN\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in eval_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy().tolist())\n",
    "        true_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "#The 11 categories are Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit.\n",
    "target_names = ['Bread', 'Dairy product', 'Dessert', 'Egg', 'Fried food', 'Meat', 'Noodles/Pasta', 'Rice', 'Seafood', 'Soup', 'Vegetable/Fruit']\n",
    "# Calculate Recall, Precision, and F1-score\n",
    "print(classification_report(true_labels, predictions, target_names=target_names))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-13T09:40:39.796267627Z",
     "start_time": "2023-09-13T09:40:29.777116111Z"
    }
   },
   "id": "58c57ac006ca2fe4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
