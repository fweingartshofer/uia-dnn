{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this task you are suppose to implement a chatbot in two ways: \n",
    "1. As a classifier \n",
    "2. As generator\n",
    "\n",
    "- Download the Python Questions from Stack Overflow dataset https://www.kaggle.com/stackoverflow/pythonquestions\n",
    "- Make the chatbot so that you classify a category (i.e., tag) of input text,\n",
    "and return a dialog from the correct class. Note that one question could\n",
    "have multiple tags and ou may need to simplify.\n",
    "- Alternatively, make a sequence to sequence network that automatically\n",
    "learns what to respond. It can be character based or word based.\n",
    "- Hint: Start with a subset of the dataset\n",
    "- Choose the network architecture with care.\n",
    "- Train and validate all algorithms.\n",
    "- Make the necessary assumptions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123fc337c60d44da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Mostly done with ChatGPT, because I don't know what I'm doing..*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce15187b4512dcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Understand and Prepare the Dataset\n",
    "\n",
    "The data consists of three files:\n",
    "\n",
    "- `Questions.csv`: Contains information about the questions asked on Stack Overflow. The 'Body' field contains the HTML of the answer.\n",
    "- `Answers.csv`: Contains information about the answers to the questions. The 'ParentId' field maps to a question.\n",
    "- `Tags.csv`: Contains the tags associated with each question. The 'Id' field here corresponds to the 'Id' in the Questions.csv file.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42808be051d30bb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Large subsets\n",
    "questions_df = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=20000)\n",
    "answers_df = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=30000)\n",
    "tags_df = pd.read_csv('data/Tags.csv', encoding='latin1')\n",
    "\n",
    "# Small subsets\n",
    "# questions_df = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=1000)\n",
    "# answers_df = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=2000)\n",
    "# tags_df = pd.read_csv('data/Tags.csv', encoding='latin1', nrows=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:07:08.227226635Z",
     "start_time": "2023-11-29T16:07:04.596339391Z"
    }
   },
   "id": "8c0f46ff6ecbbf64"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:  20000\n",
      "Answers:  30000\n",
      "Tags:  1885078\n"
     ]
    }
   ],
   "source": [
    "# give info of datasets\n",
    "print(\"Questions: \", len(questions_df))\n",
    "print(\"Answers: \", len(answers_df))\n",
    "print(\"Tags: \", len(tags_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:07:08.300430999Z",
     "start_time": "2023-11-29T16:07:08.240107073Z"
    }
   },
   "id": "a88ea17e0caf42ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "See https://www.kaggle.com/code/nicolaswattiez/stackoverflow-python-preprocess"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d201f00566f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Text Cleaning\n",
    "\n",
    "The 'Body' field in both the Questions and Answers datasets contains HTML. We need to remove these HTML tags and clean the text data. This also includes converting text to lowercase, removing punctuation, and potentially removing stop words (common words like 'is', 'the', 'and', etc., which don't add much information for the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbb3fa91be807a06"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/markus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:07:11.406226324Z",
     "start_time": "2023-11-29T16:07:08.248339470Z"
    }
   },
   "id": "caf7ae12fafc7084"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "number_of_characters = 86 # valid characters allowed from regex\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and most special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\(\\)\\{\\}\\[\\]<>:;=+\\-*/&|!.#, _@]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = \" \".join(word for word in text.split() if word not in stopwords.words('english'))\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:07:11.631469974Z",
     "start_time": "2023-11-29T16:07:11.304735992Z"
    }
   },
   "id": "a684206acd4e59ac"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "questions_df['Body'] = questions_df['Body'].apply(clean_text)\n",
    "answers_df['Body'] = answers_df['Body'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:14.395200086Z",
     "start_time": "2023-11-29T16:07:11.587398052Z"
    }
   },
   "id": "15ae9a1c76f52752"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    using photoshops javascript api find fonts giv...\n",
      "1    cross-platform (python) application needs gene...\n",
      "2    im starting work hobby project python codebase...\n",
      "3            several ways iterate result set. tradeoff\n",
      "4    dont remember whether dreaming seem recall fun...\n",
      "Name: Body, dtype: object\n",
      "0    open terminal (applications->utilities->termin...\n",
      "1    havent able find anything directly. think youl...\n",
      "2    use imagemagicks convert utility this, see exa...\n",
      "3    one possibility hudson. written java, theres i...\n",
      "4    run buildbot - trac work, havent used much sin...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(questions_df.head()['Body'])\n",
    "print(answers_df.head()['Body'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:14.414646504Z",
     "start_time": "2023-11-29T16:13:14.400016065Z"
    }
   },
   "id": "4b4688f3c71ed249"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Handling Tags:\n",
    "\n",
    "A question can have multiple tags, but for simplicity, you might want to assign it to just one category. You can choose the most frequent tag, or a tag based on the content of the question."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "618edd180f1d988b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Convert 'Tag' to string and then group by 'Id', joining all tags for a question\n",
    "grouped_tags = tags_df['Tag'].astype(str).groupby(tags_df['Id']).apply(lambda tags: ' '.join(tags))\n",
    "\n",
    "# Merge tags into questions dataframe\n",
    "questions_df = questions_df.merge(grouped_tags, how='inner', on='Id')\n",
    "\n",
    "# Function to get the most common tag\n",
    "def most_common_tag(tags):\n",
    "    tags_list = tags.split()\n",
    "    return max(set(tags_list), key = tags_list.count)\n",
    "\n",
    "# Apply function to get the most common tag for each question\n",
    "questions_df['Tag'] = questions_df['Tag'].apply(most_common_tag)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:25.231248151Z",
     "start_time": "2023-11-29T16:13:14.405573834Z"
    }
   },
   "id": "af3d397aee5376de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Tokenization:\n",
    "\n",
    "Tokenization is the process of splitting the text into individual words or tokens. This is necessary to convert your text data into a format that can be inputted into the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddeaf3879b06f075"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 17:13:25.776117: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-29 17:13:26.003275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-29 17:13:26.003308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-29 17:13:26.018397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-29 17:13:26.063735: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-29 17:13:26.064479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-29 17:13:27.417109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit it to the questions data\n",
    "tokenizer.fit_on_texts(questions_df['Body'])\n",
    "\n",
    "# Tokenize the text\n",
    "questions_df['Body'] = tokenizer.texts_to_sequences(questions_df['Body'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:30.835275423Z",
     "start_time": "2023-11-29T16:13:25.238208830Z"
    }
   },
   "id": "52c3f0c16fb13ed7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implement the classifier\n",
    "\n",
    "The idea here is to train a model to predict the tag of a question based on its content. You can use a recurrent neural network (RNN) architecture for this, as it's good at handling sequential data like text.\n",
    "You will need to convert your text and tags into numerical format for training. This can involve techniques like one-hot encoding or word embedding.\n",
    "Split your data into a training set and a validation set.\n",
    "Train your RNN on the training data and validate it on the validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfee50b0bfe2b9d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1. Prepare the Target Variable\n",
    "\n",
    "The target variable is the tag of each question. You need to convert these tags into a numerical format that can be used to train the model. One common approach is one-hot encoding, which converts each category into a binary vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b9bc8073a2cf78"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize and fit the label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(questions_df['Tag'])\n",
    "\n",
    "# Transform the tags into integers\n",
    "questions_df['Tag'] = le.transform(questions_df['Tag'])\n",
    "\n",
    "# One-hot encode the tags\n",
    "tags_encoded = to_categorical(questions_df['Tag'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:30.913817985Z",
     "start_time": "2023-11-29T16:13:30.849145661Z"
    }
   },
   "id": "89fea62441be568e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Prepare the Training and Validation Sets\n",
    "\n",
    "You need to split your data into a training set and a validation set. A common split is 80% of the data for training and 20% for validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb52b9679a4fe6e2"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(questions_df['Body'], tags_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:31.453724798Z",
     "start_time": "2023-11-29T16:13:30.915820686Z"
    }
   },
   "id": "1ad90f14a5530d2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3. Padding the Sequences\n",
    "\n",
    "Neural networks require all input to be the same length. You can use the pad_sequences function from Keras to make all sequences the same length by padding shorter ones with zeros."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f37d65814083fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences\n",
    "X_train = pad_sequences(X_train)\n",
    "X_val = pad_sequences(X_val, maxlen=X_train.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:32.303718662Z",
     "start_time": "2023-11-29T16:13:31.460896416Z"
    }
   },
   "id": "fd9cc5990ca9ec11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4. Build the LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff3ef8e40e884312"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 17:13:32.757853: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-11-29 17:13:32.797129: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 245784000 exceeds 10% of free system memory.\n",
      "2023-11-29 17:13:32.881550: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 245784000 exceeds 10% of free system memory.\n",
      "2023-11-29 17:13:32.922928: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 245784000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3953, 1000)        61446000  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               578048    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2612)              336948    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62360996 (237.89 MB)\n",
      "Trainable params: 62360996 (237.89 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=1000, input_length=X_train.shape[1]))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Add a Dense layer\n",
    "model.add(Dense(tags_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T16:13:33.438320399Z",
     "start_time": "2023-11-29T16:13:32.307400743Z"
    }
   },
   "id": "504d3b0720af9c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5. Train the Model\n",
    "\n",
    "Finally, the model can be trained using the training data, and validated using the validation data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe6dd5faf2d8e89"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 17:13:33.817076: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 252992000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 17:13:34.489378: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 167168000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 4036s 8s/step - loss: 6.4501 - accuracy: 0.0873 - val_loss: 6.2807 - val_accuracy: 0.0930\n",
      "Epoch 2/5\n",
      "279/500 [===============>..............] - ETA: 28:00 - loss: 6.0011 - accuracy: 0.0946"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, validation_data=(X_val, y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-29T17:56:21.103351212Z",
     "start_time": "2023-11-29T16:13:33.415466057Z"
    }
   },
   "id": "96b4c2c4198201f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
