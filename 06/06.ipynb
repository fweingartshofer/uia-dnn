{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this task you are suppose to implement a chatbot in two ways: \n",
    "1. As a classifier \n",
    "2. As generator\n",
    "\n",
    "- Download the Python Questions from Stack Overflow dataset https://www.kaggle.com/stackoverflow/pythonquestions\n",
    "- Make the chatbot so that you classify a category (i.e., tag) of input text,\n",
    "and return a dialog from the correct class. Note that one question could\n",
    "have multiple tags and ou may need to simplify.\n",
    "- Alternatively, make a sequence to sequence network that automatically\n",
    "learns what to respond. It can be character based or word based.\n",
    "- Hint: Start with a subset of the dataset\n",
    "- Choose the network architecture with care.\n",
    "- Train and validate all algorithms.\n",
    "- Make the necessary assumptions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123fc337c60d44da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Mostly done with ChatGPT, because I don't know what I'm doing..*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cce15187b4512dcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Understand and Prepare the Dataset\n",
    "\n",
    "The data consists of three files:\n",
    "\n",
    "- `Questions.csv`: Contains information about the questions asked on Stack Overflow. The 'Body' field contains the HTML of the answer.\n",
    "- `Answers.csv`: Contains information about the answers to the questions. The 'ParentId' field maps to a question.\n",
    "- `Tags.csv`: Contains the tags associated with each question. The 'Id' field here corresponds to the 'Id' in the Questions.csv file.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42808be051d30bb"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Large subsets\n",
    "questions_df = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=10000)\n",
    "answers_df = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=15000)\n",
    "tags_df = pd.read_csv('data/Tags.csv', encoding='latin1')\n",
    "\n",
    "# Small subsets\n",
    "# questions_df = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=1000)\n",
    "# answers_df = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=2000)\n",
    "# tags_df = pd.read_csv('data/Tags.csv', encoding='latin1', nrows=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:18:35.680122161Z",
     "start_time": "2023-11-24T21:18:34.959454559Z"
    }
   },
   "id": "8c0f46ff6ecbbf64"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:  10000\n",
      "Answers:  15000\n",
      "Tags:  1885078\n"
     ]
    }
   ],
   "source": [
    "# give info of datasets\n",
    "print(\"Questions: \", len(questions_df))\n",
    "print(\"Answers: \", len(answers_df))\n",
    "print(\"Tags: \", len(tags_df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:18:35.680524882Z",
     "start_time": "2023-11-24T21:18:35.642690045Z"
    }
   },
   "id": "a88ea17e0caf42ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "See https://www.kaggle.com/code/nicolaswattiez/stackoverflow-python-preprocess"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d201f00566f013"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. Text Cleaning\n",
    "\n",
    "The 'Body' field in both the Questions and Answers datasets contains HTML. We need to remove these HTML tags and clean the text data. This also includes converting text to lowercase, removing punctuation, and potentially removing stop words (common words like 'is', 'the', 'and', etc., which don't add much information for the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbb3fa91be807a06"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/markus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:18:35.844438974Z",
     "start_time": "2023-11-24T21:18:35.642941253Z"
    }
   },
   "id": "caf7ae12fafc7084"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "i = i\n",
    "\n",
    "def clean_text(text):\n",
    "    global i\n",
    "    i = i + 1\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and most special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\(\\)\\{\\}\\[\\]<>:;=+\\-*/&|!.#, _@]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = \" \".join(word for word in text.split() if word not in stopwords.words('english'))\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:18:35.973682531Z",
     "start_time": "2023-11-24T21:18:35.850915868Z"
    }
   },
   "id": "a684206acd4e59ac"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m questions_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBody\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m questions_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBody\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(clean_text)\n\u001B[0;32m----> 2\u001B[0m answers_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBody\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43manswers_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mBody\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclean_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/series.py:4760\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4626\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4627\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4632\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4633\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4634\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4635\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4636\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4751\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4752\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4754\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4756\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4757\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4758\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4760\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/apply.py:1207\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1207\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/apply.py:1287\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1281\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1285\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1286\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1287\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1288\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1289\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1291\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1292\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1293\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1812\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1813\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1814\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1815\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1816\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1817\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1818\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2920\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[60], line 17\u001B[0m, in \u001B[0;36mclean_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     15\u001B[0m text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^a-zA-Z0-9\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Remove stopwords\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stopwords\u001B[38;5;241m.\u001B[39mwords(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "Cell \u001B[0;32mIn[60], line 17\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     15\u001B[0m text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^a-zA-Z0-9\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# Remove stopwords\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(word \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m text\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mstopwords\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43menglish\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/nltk/corpus/reader/wordlist.py:21\u001B[0m, in \u001B[0;36mWordListCorpusReader.words\u001B[0;34m(self, fileids, ignore_lines_startswith)\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwords\u001B[39m(\u001B[38;5;28mself\u001B[39m, fileids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, ignore_lines_startswith\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m     20\u001B[0m         line\n\u001B[0;32m---> 21\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m line_tokenize(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfileids\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     22\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m line\u001B[38;5;241m.\u001B[39mstartswith(ignore_lines_startswith)\n\u001B[1;32m     23\u001B[0m     ]\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/nltk/corpus/reader/api.py:218\u001B[0m, in \u001B[0;36mCorpusReader.raw\u001B[0;34m(self, fileids)\u001B[0m\n\u001B[1;32m    216\u001B[0m contents \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fileids:\n\u001B[0;32m--> 218\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    219\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcontents\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mappend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concat(contents)\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/nltk/data.py:1166\u001B[0m, in \u001B[0;36mSeekableUnicodeStreamReader.__exit__\u001B[0;34m(self, type, value, traceback)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__enter__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m   1164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[0;32m-> 1166\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__exit__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mtype\u001B[39m, value, traceback):\n\u001B[1;32m   1167\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mxreadlines\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "questions_df['Body'] = questions_df['Body'].apply(clean_text)\n",
    "answers_df['Body'] = answers_df['Body'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T21:20:29.878761644Z",
     "start_time": "2023-11-24T21:18:35.893312703Z"
    }
   },
   "id": "15ae9a1c76f52752"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(questions_df.head()['Body'])\n",
    "print(answers_df.head()['Body'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859554865Z"
    }
   },
   "id": "4b4688f3c71ed249"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. Handling Tags:\n",
    "\n",
    "A question can have multiple tags, but for simplicity, you might want to assign it to just one category. You can choose the most frequent tag, or a tag based on the content of the question."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "618edd180f1d988b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Group by 'Id' and join all tags for a question\n",
    "grouped_tags = tags_df.groupby(\"Id\")['Tag'].apply(lambda tags: ' '.join(tags))\n",
    "\n",
    "# Merge tags into questions dataframe\n",
    "questions_df = questions_df.merge(grouped_tags, how='inner', on='Id')\n",
    "\n",
    "# Function to get the most common tag\n",
    "def most_common_tag(tags):\n",
    "    tags_list = tags.split()\n",
    "    return max(set(tags_list), key = tags_list.count)\n",
    "\n",
    "# Apply function to get the most common tag for each question\n",
    "questions_df['Tag'] = questions_df['Tag'].apply(most_common_tag)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859714157Z"
    }
   },
   "id": "af3d397aee5376de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3. Tokenization:\n",
    "\n",
    "Tokenization is the process of splitting the text into individual words or tokens. This is necessary to convert your text data into a format that can be inputted into the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddeaf3879b06f075"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit it to the questions data\n",
    "tokenizer.fit_on_texts(questions_df['Body'])\n",
    "\n",
    "# Tokenize the text\n",
    "questions_df['Body'] = tokenizer.texts_to_sequences(questions_df['Body'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859784732Z"
    }
   },
   "id": "52c3f0c16fb13ed7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implement the classifier\n",
    "\n",
    "The idea here is to train a model to predict the tag of a question based on its content. You can use a recurrent neural network (RNN) architecture for this, as it's good at handling sequential data like text.\n",
    "You will need to convert your text and tags into numerical format for training. This can involve techniques like one-hot encoding or word embedding.\n",
    "Split your data into a training set and a validation set.\n",
    "Train your RNN on the training data and validate it on the validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfee50b0bfe2b9d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1. Prepare the Target Variable\n",
    "\n",
    "The target variable is the tag of each question. You need to convert these tags into a numerical format that can be used to train the model. One common approach is one-hot encoding, which converts each category into a binary vector."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b9bc8073a2cf78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize and fit the label encoder\n",
    "le = LabelEncoder()\n",
    "le.fit(questions_df['Tag'])\n",
    "\n",
    "# Transform the tags into integers\n",
    "questions_df['Tag'] = le.transform(questions_df['Tag'])\n",
    "\n",
    "# One-hot encode the tags\n",
    "tags_encoded = to_categorical(questions_df['Tag'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859846069Z"
    }
   },
   "id": "89fea62441be568e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2. Prepare the Training and Validation Sets\n",
    "\n",
    "You need to split your data into a training set and a validation set. A common split is 80% of the data for training and 20% for validation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb52b9679a4fe6e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(questions_df['Body'], tags_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859905164Z"
    }
   },
   "id": "1ad90f14a5530d2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3. Padding the Sequences\n",
    "\n",
    "Neural networks require all input to be the same length. You can use the pad_sequences function from Keras to make all sequences the same length by padding shorter ones with zeros."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f37d65814083fc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences\n",
    "X_train = pad_sequences(X_train)\n",
    "X_val = pad_sequences(X_val, maxlen=X_train.shape[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859950830Z"
    }
   },
   "id": "fd9cc5990ca9ec11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4. Build the LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff3ef8e40e884312"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an Embedding layer\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=X_train.shape[1]))\n",
    "\n",
    "# Add an LSTM layer\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Add a Dense layer\n",
    "model.add(Dense(tags_encoded.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.859995974Z"
    }
   },
   "id": "504d3b0720af9c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5. Train the Model\n",
    "\n",
    "Finally, the model can be trained using the training data, and validated using the validation data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe6dd5faf2d8e89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-24T21:20:29.860039469Z"
    }
   },
   "id": "96b4c2c4198201f8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
