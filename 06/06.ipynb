{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this task you are supposed to implement a chatbot in two ways: \n",
    "1. As a classifier \n",
    "2. As generator\n",
    "\n",
    "- Download the Python Questions from Stack Overflow dataset https://www.kaggle.com/stackoverflow/pythonquestions\n",
    "- Make the chatbot so that you classify a category (i.e., tag) of input text,\n",
    "and return a dialog from the correct class. Note that one question could\n",
    "have multiple tags and ou may need to simplify.\n",
    "- Alternatively, make a sequence to sequence network that automatically\n",
    "learns what to respond. It can be character based or word based.\n",
    "- Hint: Start with a subset of the dataset\n",
    "- Choose the network architecture with care.\n",
    "- Train and validate all algorithms.\n",
    "- Make the necessary assumptions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "123fc337c60d44da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. As a Classifier\n",
    "\n",
    "To create a chatbot as a classifier, we will classify the input text to a category (tag) and return a dialog from that tag.\n",
    "\n",
    "### Understand and Prepare the Dataset\n",
    "\n",
    "The data consists of three files:\n",
    "\n",
    "- `Questions.csv`: Contains information about the questions asked on Stack Overflow. The 'Body' field contains the HTML of the answer.\n",
    "- `Answers.csv`: Contains information about the answers to the questions. The 'ParentId' field maps to a question.\n",
    "- `Tags.csv`: Contains the tags associated with each question. The 'Id' field here corresponds to the 'Id' in the Questions.csv file.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42808be051d30bb"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Large subsets\n",
    "questions = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=10000)\n",
    "answers = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=15000)\n",
    "tags = pd.read_csv('data/Tags.csv', encoding='latin1')\n",
    "\n",
    "# Small subsets\n",
    "# questions = pd.read_csv('data/Questions.csv', encoding='latin1', nrows=1000)\n",
    "# answers = pd.read_csv('data/Answers.csv', encoding='latin1', nrows=2000)\n",
    "# tags = pd.read_csv('data/Tags.csv', encoding='latin1', nrows=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:37:30.413928330Z",
     "start_time": "2023-12-02T21:37:29.341389943Z"
    }
   },
   "id": "8c0f46ff6ecbbf64"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:  10000\n",
      "Answers:  15000\n",
      "Tags:  1885078\n"
     ]
    }
   ],
   "source": [
    "# give info of datasets\n",
    "print(\"Questions: \", len(questions))\n",
    "print(\"Answers: \", len(answers))\n",
    "print(\"Tags: \", len(tags))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:37:30.421284019Z",
     "start_time": "2023-12-02T21:37:30.417374041Z"
    }
   },
   "id": "a88ea17e0caf42ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "First, we need to preprocess our data. We will use the pandas library to load the data and BeautifulSoup library to clean it.\n",
    "\n",
    "See https://www.kaggle.com/code/nicolaswattiez/stackoverflow-python-preprocess\n",
    "\n",
    "The 'Body' field in both the Questions and Answers datasets contains HTML. We need to remove these HTML tags and clean the text data. This also includes converting text to lowercase, removing punctuation, and potentially removing stop words (common words like 'is', 'the', 'and', etc., which don't add much information for the model)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d201f00566f013"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/markus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:37:31.846118065Z",
     "start_time": "2023-12-02T21:37:30.421622900Z"
    }
   },
   "id": "caf7ae12fafc7084"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "number_of_characters = 86 # valid characters allowed from regex\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and most special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\(\\)\\{\\}\\[\\]<>:;=+\\-*/&|!.#, _@]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = \" \".join(word for word in text.split() if word not in stopwords.words('english'))\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:37:31.971160552Z",
     "start_time": "2023-12-02T21:37:31.851270907Z"
    }
   },
   "id": "a684206acd4e59ac"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "questions['Body'] = questions['Body'].apply(clean_text)\n",
    "answers['Body'] = answers['Body'].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:20.245483246Z",
     "start_time": "2023-12-02T21:37:31.966339752Z"
    }
   },
   "id": "15ae9a1c76f52752"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    using photoshops javascript api find fonts giv...\n",
      "1    cross-platform (python) application needs gene...\n",
      "2    im starting work hobby project python codebase...\n",
      "3            several ways iterate result set. tradeoff\n",
      "4    dont remember whether dreaming seem recall fun...\n",
      "Name: Body, dtype: object\n",
      "0    open terminal (applications->utilities->termin...\n",
      "1    havent able find anything directly. think youl...\n",
      "2    use imagemagicks convert utility this, see exa...\n",
      "3    one possibility hudson. written java, theres i...\n",
      "4    run buildbot - trac work, havent used much sin...\n",
      "Name: Body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(questions.head()['Body'])\n",
    "print(answers.head()['Body'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:20.289899086Z",
     "start_time": "2023-12-02T21:40:20.287991480Z"
    }
   },
   "id": "4b4688f3c71ed249"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simplify the tags\n",
    "\n",
    "As a question can have multiple tags, we need to simplify this. We can either choose one tag per question or create a multi-label classifier. For simplicity, we will choose the first tag for each question."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "618edd180f1d988b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Group the tags by question ID\n",
    "tags = tags.groupby('Id').first().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:20.537255997Z",
     "start_time": "2023-12-02T21:40:20.288825269Z"
    }
   },
   "id": "af3d397aee5376de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Merge the data\n",
    "\n",
    "We'll merge the questions and tags dataframes on the 'Id' field."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ef4563d0dae1ce4"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Merge tags into questions dataframe\n",
    "data = pd.merge(questions, tags, how='inner', on='Id')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:20.595125590Z",
     "start_time": "2023-12-02T21:40:20.542629684Z"
    }
   },
   "id": "b7fcf15fc09a7f51"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create and Train the Classifier\n",
    "\n",
    "We'll use a simple text classifier. For this, we can use the TF-IDF vectorizer to transform our text data into numerical data and then use a classifier like logistic regression."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfee50b0bfe2b9d8"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Body'], data['Tag'], test_size=0.2)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Validate the classifier\n",
    "print(f\"Accuracy: {pipeline.score(X_test, y_test)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:44.884601496Z",
     "start_time": "2023-12-02T21:40:20.599655832Z"
    }
   },
   "id": "1ad90f14a5530d2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the Classifier\n",
    "\n",
    "Now that we have trained our classifier, we can use it to classify new text and return a dialog from the corresponding tag."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f37d65814083fc9"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_response(text):\n",
    "    # Predict the tag\n",
    "    tag = pipeline.predict([text])[0]\n",
    "    \n",
    "    # Get a random dialog from the tag\n",
    "    response = data[data['Tag'] == tag]['Body'].sample(1).values[0]\n",
    "    \n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:44.884831890Z",
     "start_time": "2023-12-02T21:40:44.884441837Z"
    }
   },
   "id": "fd9cc5990ca9ec11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. As a Generator\n",
    "\n",
    "Creating a chatbot as a generator is a bit more complex, as we need to create a sequence-to-sequence (seq2seq) network. We'll use the Keras library for this."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff3ef8e40e884312"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "The preprocessing steps are similar to the classifier, but we need to tokenize our text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1d6e97cabdccf28"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 22:40:45.269322: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 22:40:45.391731: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-02 22:40:45.391761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-02 22:40:45.405612: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-02 22:40:45.439506: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-02 22:40:45.440342: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-02 22:40:46.670293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['Body'])\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(data['Body'])\n",
    "\n",
    "# Pad sequences\n",
    "sequences = pad_sequences(sequences)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:48.580085855Z",
     "start_time": "2023-12-02T21:40:44.884773871Z"
    }
   },
   "id": "504d3b0720af9c76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the Seq2Seq model\n",
    "\n",
    "We'll create a simple seq2seq model with LSTM layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbe6dd5faf2d8e89"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 22:40:48.868865: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional, got ndarray of shape (10000, 1868) instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(sequences, \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dummies\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mvalues, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:221\u001B[0m, in \u001B[0;36mget_dummies\u001B[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001B[0m\n\u001B[1;32m    219\u001B[0m     result \u001B[38;5;241m=\u001B[39m concat(with_dummies, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 221\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_get_dummies_1d\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    222\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    223\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    224\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprefix_sep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdummy_na\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    226\u001B[0m \u001B[43m        \u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdrop_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    228\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    229\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:245\u001B[0m, in \u001B[0;36m_get_dummies_1d\u001B[0;34m(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)\u001B[0m\n\u001B[1;32m    242\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconcat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m concat\n\u001B[1;32m    244\u001B[0m \u001B[38;5;66;03m# Series avoids inconsistent NaN handling\u001B[39;00m\n\u001B[0;32m--> 245\u001B[0m codes, levels \u001B[38;5;241m=\u001B[39m factorize_from_iterable(\u001B[43mSeries\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    248\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;28mbool\u001B[39m)\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/series.py:512\u001B[0m, in \u001B[0;36mSeries.__init__\u001B[0;34m(self, data, index, dtype, name, copy, fastpath)\u001B[0m\n\u001B[1;32m    510\u001B[0m         data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 512\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43msanitize_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    514\u001B[0m     manager \u001B[38;5;241m=\u001B[39m get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.data_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mblock\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/construction.py:646\u001B[0m, in \u001B[0;36msanitize_array\u001B[0;34m(data, index, dtype, copy, allow_2d)\u001B[0m\n\u001B[1;32m    643\u001B[0m             subarr \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mndarray, subarr)\n\u001B[1;32m    644\u001B[0m             subarr \u001B[38;5;241m=\u001B[39m maybe_infer_to_datetimelike(subarr)\n\u001B[0;32m--> 646\u001B[0m subarr \u001B[38;5;241m=\u001B[39m \u001B[43m_sanitize_ndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_2d\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_2d\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    648\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(subarr, np\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[1;32m    649\u001B[0m     \u001B[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001B[39;00m\n\u001B[1;32m    650\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m cast(np\u001B[38;5;241m.\u001B[39mdtype, dtype)\n",
      "File \u001B[0;32m~/fh/dnn/lab/venv/lib/python3.11/site-packages/pandas/core/construction.py:705\u001B[0m, in \u001B[0;36m_sanitize_ndim\u001B[0;34m(result, data, dtype, index, allow_2d)\u001B[0m\n\u001B[1;32m    703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m allow_2d:\n\u001B[1;32m    704\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n\u001B[0;32m--> 705\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    706\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData must be 1-dimensional, got ndarray of shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m instead\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    707\u001B[0m     )\n\u001B[1;32m    708\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_object_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dtype, ExtensionDtype):\n\u001B[1;32m    709\u001B[0m     \u001B[38;5;66;03m# i.e. NumpyEADtype(\"O\")\u001B[39;00m\n\u001B[1;32m    711\u001B[0m     result \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39masarray_tuplesafe(data, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mdtype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobject\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mValueError\u001B[0m: Data must be 1-dimensional, got ndarray of shape (10000, 1868) instead"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256, input_length=sequences.shape[1]))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(sequences, pd.get_dummies(sequences).values, epochs=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T21:40:50.200319934Z",
     "start_time": "2023-12-02T21:40:48.583215129Z"
    }
   },
   "id": "96b4c2c4198201f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use the Seq2Seq model\n",
    "\n",
    "We can use the seq2seq model to generate new text based on the input text."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83faa858d26afc33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    # Convert text to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    \n",
    "    # Predict next word\n",
    "    prediction = model.predict_classes(sequence)\n",
    "    \n",
    "    # Get word from index\n",
    "    word = tokenizer.index_word[prediction[0]]\n",
    "    \n",
    "    return word"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-02T21:40:50.244264020Z"
    }
   },
   "id": "b487437a67f6abed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
